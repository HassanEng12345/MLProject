{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3734fc3",
   "metadata": {},
   "source": [
    "## ML Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f929c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc9132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Bank.csv\")\n",
    "\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_check = df.notnull().all()\n",
    "print(\"Non-null check for each column:\")\n",
    "print(non_null_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'credit_score' for churned and non-churned customers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(x='credit_score', data=df, hue='churn', kde=True, multiple=\"stack\")\n",
    "plt.title('Distribution of Credit Score by Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e67e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot countplot for 'gender' against 'churn'\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='gender', hue='churn', data=df)\n",
    "plt.title('Gender Distribution by Churn')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'age' for churned and non-churned customers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(x='age', data=df, hue='churn', kde=True, multiple=\"stack\")\n",
    "plt.title('Distribution of Age by Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f621a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'balance' for churned and non-churned customers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(x='balance', data=df, hue='churn', kde=True, multiple=\"stack\")\n",
    "plt.title('Distribution of Balance by Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67289928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'products_number' for churned and non-churned customers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='products_number', hue='churn', data=df)\n",
    "plt.title('Distribution of Products Number by Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c01157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'credit_card' for churned and non-churned customers\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='credit_card', hue='churn', data=df)\n",
    "plt.title('Distribution of Credit Card Holders by Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb867249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'active_member' for churned and non-churned customers\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='active_member', hue='churn', data=df)\n",
    "plt.title('Distribution of Active Members by Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2d1239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of 'estimated_salary' for churned and non-churned customers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(x='estimated_salary', data=df, hue='churn', kde=True, multiple=\"stack\")\n",
    "plt.title('Estimated Salary Distribution by Churn')\n",
    "plt.xlabel('Estimated Salary')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['credit_score', 'age', 'balance', 'products_number', 'credit_card', 'active_member','estimated_salary', 'churn']\n",
    "numeric_df = df[numeric_cols]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix with Churn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c8062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with the target variable\n",
    "correlation_with_churn = df[numeric_cols].corr()['churn'].sort_values(ascending=False)\n",
    "\n",
    "# Plot correlation with the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlation_with_churn.drop('churn').plot(kind='bar')\n",
    "plt.title('Correlation of Numeric Features with Churn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa027b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, chi2_contingency\n",
    "\n",
    "features_to_analyze = ['credit_score', 'age', 'balance', 'products_number', 'estimated_salary', 'gender', 'credit_card', 'active_member']\n",
    "results = []\n",
    "for feature in features_to_analyze:\n",
    "    if df[feature].dtype == 'float64' or df[feature].dtype == 'int64': \n",
    "        churned_numeric = df[df['churn'] == 1][feature]\n",
    "        non_churned_numeric = df[df['churn'] == 0][feature]\n",
    "        t_stat, p_value = ttest_ind(churned_numeric, non_churned_numeric)\n",
    "        result = {'Feature': feature, 'Test': 't-test', 'Stat': t_stat, 'P-value': p_value}\n",
    "        results.append(result)\n",
    "    elif df[feature].dtype == 'object':\n",
    "        contingency_table = pd.crosstab(df[feature], df['churn'])\n",
    "        chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "        result = {'Feature': feature, 'Test': 'Chi-squared test', 'Stat': chi2_stat, 'P-value': p_value}\n",
    "        results.append(result)\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65222bcf",
   "metadata": {},
   "source": [
    "## Binary logistic regeression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9971ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_dataset = \"Bank.csv\"\n",
    "#implement age variable which is located at the 4th index\n",
    "age=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[4]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_vector=age\n",
    "print(age_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the target varible\n",
    "churn=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[11]).astype(int)\n",
    "churn_vector=churn\n",
    "print(churn_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901cd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "activeMem=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[9]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_matrix(data_input):\n",
    "    data_matrix = np.c_[np.ones(data_input.shape[0]), data_input]\n",
    "    return data_matrix\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def model_function(data_matrix, weights):\n",
    "    return data_matrix @ weights.squeeze()\n",
    "\n",
    "def binary_logistic_regression_cost_function(data_matrix, data_labels, weights):\n",
    "    regression_outputs = sigmoid(model_function(data_matrix, weights))\n",
    "    return np.mean(-data_labels * np.log(regression_outputs) - (1 - data_labels) * np.log(1 - regression_outputs))\n",
    "\n",
    "def binary_logistic_regression_gradient(data_matrix, data_labels, weights):\n",
    "    regression_outputs = sigmoid(model_function(data_matrix, weights))\n",
    "    error = regression_outputs - data_labels\n",
    "    gradient = data_matrix.T @ error / len(data_labels)\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent(objective, gradient, initial_weights, step_size, no_of_iterations, print_output=None):\n",
    "    objective_values = []\n",
    "    weights = np.copy(initial_weights)\n",
    "\n",
    "    for counter in range(no_of_iterations):\n",
    "        weights -= step_size * gradient(weights)\n",
    "\n",
    "        if print_output is not None and counter % print_output == 0:\n",
    "            current_value = objective(weights)\n",
    "            objective_values.append(current_value)\n",
    "            print(f\"Iteration {counter}, Objective Value: {current_value}\")\n",
    "\n",
    "    return weights, objective_values\n",
    "\n",
    "def train_logistic_regression(data_matrix, data_labels, initial_weights, step_size, epochs=1000):\n",
    "    objective = lambda weights: binary_logistic_regression_cost_function(data_matrix, data_labels, weights)\n",
    "    gradient = lambda weights: binary_logistic_regression_gradient(data_matrix, data_labels, weights)\n",
    "\n",
    "    trained_weights, _ = gradient_descent(objective, gradient, initial_weights, step_size, epochs, print_output=100)\n",
    "\n",
    "    return trained_weights\n",
    "\n",
    "def KFold_split(data_size, K):\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K)\n",
    "    indexes_split = [indexes[i * m + min(i, r):(i + 1) * m + min(i + 1, r)] for i in range(K)]\n",
    "    return indexes_split\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == np.round(y_pred))\n",
    "\n",
    "def KFold_cross_validation(data_matrix, data_outputs, K, model_evaluation, error_evaluation):\n",
    "    indexes_split = KFold_split(len(data_matrix), K)\n",
    "    total_error = 0\n",
    "    total_accuracy = 0\n",
    "\n",
    "    for i in range(K):\n",
    "        indexes = np.concatenate([indexes_split[j] for j in range(K) if (j != i)])\n",
    "        weights = model_evaluation(data_matrix[indexes], data_outputs[indexes])\n",
    "        predictions = sigmoid(model_function(data_matrix[indexes_split[i]], weights))\n",
    "        error = error_evaluation(data_matrix[indexes_split[i]], data_outputs[indexes_split[i]], weights)\n",
    "\n",
    "        total_error += error\n",
    "        total_accuracy += calculate_accuracy(data_outputs[indexes_split[i]], predictions)\n",
    "\n",
    "    optimal_weights = model_evaluation(data_matrix, data_outputs)\n",
    "\n",
    "    return optimal_weights, total_error / K, total_accuracy / K\n",
    "\n",
    "\n",
    "data_matrix = augment_data_matrix(age_vector)\n",
    "\n",
    "\n",
    "initial_weights = np.zeros(data_matrix.shape[1])\n",
    "\n",
    "\n",
    "step_size = 3.9 * len(age_vector) / (np.linalg.norm(age_vector))**2\n",
    "\n",
    "\n",
    "optimal_weights, average_validation_error, average_accuracy = KFold_cross_validation(data_matrix, churn_vector, K=5,\n",
    "                                                           model_evaluation=lambda x, y: train_logistic_regression(x, y, initial_weights, step_size),\n",
    "                                                           error_evaluation=binary_logistic_regression_cost_function)\n",
    "\n",
    "print(f\"Optimal Weights: {optimal_weights}\")\n",
    "print(f\"Average Validation Error: {average_validation_error}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052e0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_matrix = augment_data_matrix(activeMem)\n",
    "\n",
    "initial_weights = np.zeros(data_matrix.shape[1])\n",
    "\n",
    "\n",
    "step_size = 3.9 * len(activeMem) / (np.linalg.norm(activeMem))**2\n",
    "\n",
    "optimal_weights, average_validation_error, average_accuracy = KFold_cross_validation(data_matrix, churn_vector, K=5,\n",
    "                                                           model_evaluation=lambda x, y: train_logistic_regression(x, y, initial_weights, step_size),\n",
    "                                                           error_evaluation=binary_logistic_regression_cost_function)\n",
    "\n",
    "print(f\"Optimal Weights: {optimal_weights}\")\n",
    "print(f\"Average Validation Error: {average_validation_error}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_2Variables=np.c_[age,activeMem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9881540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise(data_matrix):\n",
    "    \n",
    "    row_of_means = np.mean(data_matrix, axis=0)\n",
    "    standardised_matrix = data_matrix - row_of_means\n",
    "    row_of_stds = np.std(data_matrix, axis=0)\n",
    "    \n",
    "    return (standardised_matrix / row_of_stds), row_of_means, row_of_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9322af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_2Variables,data_row_of_means, data_row_of_stds=standardise(data_inputs_2Variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3085564",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_matrix = augment_data_matrix(data_inputs_2Variables)\n",
    "\n",
    "initial_weights = np.zeros(data_matrix.shape[1])\n",
    "\n",
    "\n",
    "step_size = 3.9 * len(data_inputs_2Variables) / (np.linalg.norm(data_inputs_2Variables))**2\n",
    "\n",
    "\n",
    "optimal_weights, average_validation_error, average_accuracy = KFold_cross_validation(data_matrix, churn_vector, K=5,\n",
    "                                                           model_evaluation=lambda x, y: train_logistic_regression(x, y, initial_weights, step_size),\n",
    "                                                           error_evaluation=binary_logistic_regression_cost_function)\n",
    "\n",
    "print(f\"Optimal Weights: {optimal_weights}\")\n",
    "print(f\"Average Validation Error: {average_validation_error}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f1760f",
   "metadata": {},
   "source": [
    "### Test again for other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "balance=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[6]).astype(float)\n",
    "productsNum=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[7]).astype(float)\n",
    "gender=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[3],\\\n",
    "                    converters={3:lambda x:0 if x==b'Male' else 1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27869b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs=np.c_[age,balance,productsNum,activeMem,gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_matrix(matrix):\n",
    "    row_of_means = np.nanmean(matrix, axis=1)\n",
    "    row_of_stds = np.nanstd(matrix, axis=1)\n",
    "    zero_std_mask = row_of_stds == 0\n",
    "    row_of_stds[zero_std_mask] = 1 \n",
    "\n",
    "    standardised_matrix = (matrix - row_of_means[:, np.newaxis]) / row_of_stds[:, np.newaxis]\n",
    "    return standardised_matrix, row_of_means, row_of_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs,data_row_of_means, data_row_of_stds=standardise(data_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_matrix = augment_data_matrix(data_inputs)\n",
    "\n",
    "initial_weights = np.zeros(data_matrix.shape[1])\n",
    "\n",
    "step_size = 3.9 * len(data_inputs) / (np.linalg.norm(data_inputs))**2\n",
    "\n",
    "optimal_weights, average_validation_error, average_accuracy = KFold_cross_validation(data_matrix, churn_vector, K=5,\n",
    "                                                           model_evaluation=lambda x, y: train_logistic_regression(x, y, initial_weights, step_size),\n",
    "                                                           error_evaluation=binary_logistic_regression_cost_function)\n",
    "\n",
    "print(f\"Optimal Weights: {optimal_weights}\")\n",
    "print(f\"Average Validation Error: {average_validation_error}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9238ce8",
   "metadata": {},
   "source": [
    "### Do for all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "creditScore=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[1]).astype(int)\n",
    "estimatedSalary=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[10]).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f90271",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_final=np.c_[age,balance,productsNum,activeMem,creditScore,estimatedSalary,gender]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619686db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_matrix(matrix):\n",
    "    row_of_means = np.nanmean(matrix, axis=1)\n",
    "    row_of_stds = np.nanstd(matrix, axis=1)\n",
    "    zero_std_mask = row_of_stds == 0\n",
    "    row_of_stds[zero_std_mask] = 1\n",
    "    standardised_matrix = (matrix - row_of_means[:, np.newaxis]) / row_of_stds[:, np.newaxis]\n",
    "    return standardised_matrix, row_of_means, row_of_stds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208fc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_final,data_row_of_means, data_row_of_stds=standardise(data_inputs_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_matrix = augment_data_matrix(data_inputs_final)\n",
    "\n",
    "\n",
    "initial_weights = np.zeros(data_matrix.shape[1])\n",
    "\n",
    "\n",
    "step_size = 3.9 * len(data_inputs_final) / (np.linalg.norm(data_inputs_final))**2\n",
    "\n",
    "optimal_weights, average_validation_error, average_accuracy = KFold_cross_validation(data_matrix, churn_vector, K=5,\n",
    "                                                           model_evaluation=lambda x, y: train_logistic_regression(x, y, initial_weights, step_size),\n",
    "                                                           error_evaluation=binary_logistic_regression_cost_function)\n",
    "\n",
    "print(f\"Optimal Weights: {optimal_weights}\")\n",
    "print(f\"Average Validation Error: {average_validation_error}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac331780",
   "metadata": {},
   "source": [
    "### Now do nearset neihbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183576c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(from_data, to_data):\n",
    "    return np.sqrt(np.sum((from_data[:, np.newaxis] - to_data[np.newaxis, :])**2, axis=1))\n",
    "\n",
    "def nearest_neighbour_classification(testing_inputs, training_inputs, training_outputs, no_of_neighbours):\n",
    "    if testing_inputs.ndim == 1:\n",
    "        testing_inputs = testing_inputs.reshape(-1, 1)\n",
    "\n",
    "    distances = pairwise_distances(testing_inputs, training_inputs)\n",
    "    no_of_classes = 1 + np.max(training_outputs)\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "\n",
    "    no_of_inputs = len(testing_inputs)\n",
    "    no_of_points = len(training_inputs)\n",
    "\n",
    "    new_array_to_sort = np.broadcast_to(training_outputs, (no_of_inputs, no_of_points))\n",
    "    sorted_labels = np.take_along_axis(new_array_to_sort, sorted_indices, 1)\n",
    "\n",
    "    predicted_labels = np.zeros(no_of_inputs, int)\n",
    "    for id_input in range(no_of_inputs):\n",
    "        probability_labels = np.zeros(no_of_classes, float)\n",
    "\n",
    "        for id_neighbor in range(0, no_of_neighbours):\n",
    "            probability_labels[sorted_labels[id_input][id_neighbor]] += 1. / no_of_neighbours\n",
    "\n",
    "        predicted_labels[id_input] = np.argmax(probability_labels)\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "def classification_accuracy(estimated_labels, true_labels):\n",
    "    equal_labels = estimated_labels == true_labels\n",
    "    return np.mean(equal_labels)\n",
    "\n",
    "def KFold_split(data_size, K):\n",
    "    np.random.seed(123456789)\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K)\n",
    "    indexes_split = [\n",
    "        indexes[i * m + min(i, r):(i + 1) * m + min(i + 1, r)]\n",
    "        for i in range(K)\n",
    "    ]\n",
    "    return indexes_split\n",
    "\n",
    "def KFold_cross_validation_knn(data_inputs, data_outputs, K, labels_evaluation,\n",
    "                               missclassification_evaluation, knn):\n",
    "    data_size = len(data_inputs)\n",
    "    indexes_split = KFold_split(data_size, K)\n",
    "\n",
    "    average_accuracy = 0\n",
    "    for i in range(K):\n",
    "        training_indexes = np.concatenate([indexes_split[j] for j in range(K) if (j != i)])\n",
    "        \n",
    "        predicted_labels = labels_evaluation(data_inputs[indexes_split[i]],\n",
    "                                             data_inputs[training_indexes],\n",
    "                                             data_outputs[training_indexes], knn)\n",
    "        \n",
    "        accuracy = missclassification_evaluation(predicted_labels,\n",
    "                                                data_outputs[indexes_split[i]])\n",
    "        average_accuracy += accuracy / K\n",
    "        \n",
    "    error = 1. - average_accuracy\n",
    "    return error, 1. - error \n",
    "\n",
    "def grid_search(objective, grid):\n",
    "    values = np.array([])\n",
    "    for point in grid:\n",
    "        values = np.append(values, objective(point))\n",
    "    return grid[np.argmin(values)]\n",
    "\n",
    "def objective_fun(Knn, data_size, age_vector, churn_vector, K=5):\n",
    "    indexes_split = KFold_split(data_size, K)\n",
    "\n",
    "    average_accuracy = 0\n",
    "    for i in range(K):\n",
    "        training_indexes = np.concatenate([indexes_split[j] for j in range(K) if (j != i)])\n",
    "\n",
    "        predicted_labels = nearest_neighbour_classification(age_vector[indexes_split[i]],\n",
    "                                                            age_vector[training_indexes],\n",
    "                                                            churn_vector[training_indexes], Knn)\n",
    "\n",
    "        accuracy = classification_accuracy(predicted_labels, churn_vector[indexes_split[i]])\n",
    "        average_accuracy += accuracy / K\n",
    "        \n",
    "    error = 1. - average_accuracy\n",
    "    return error\n",
    "\n",
    "def run_knn_with_cross_validation(data_size, age_vector, churn_vector, K=5):\n",
    "    k_values = np.arange(1, 10)\n",
    "    best_k = grid_search(lambda knn: objective_fun(knn, data_size, age_vector, churn_vector, K=5), k_values)\n",
    "    cross_validation_errors = []\n",
    "    accuracies = []\n",
    "\n",
    "    for _ in range(10):\n",
    "        error, accuracy = KFold_cross_validation_knn(age_vector, churn_vector, K, nearest_neighbour_classification,\n",
    "                                                     classification_accuracy, best_k)\n",
    "        cross_validation_errors.append(error)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    average_error = np.mean(cross_validation_errors)\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "\n",
    "    return best_k, average_error, average_accuracy\n",
    "\n",
    "\n",
    "data_size = len(age_vector)\n",
    "best_k, average_error, average_accuracy = run_knn_with_cross_validation(data_size, age_vector, churn_vector, K=5)\n",
    "\n",
    "print(f'The best value for K is: {best_k}')\n",
    "print(f'Average Cross Validation Error: {average_error}, Average Accuracy: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fd3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(activeMem)\n",
    "\n",
    "best_k, average_error, average_accuracy = run_knn_with_cross_validation(data_size, activeMem, churn_vector, K=5)\n",
    "\n",
    "print(f'The best value for K is: {best_k}')\n",
    "print(f'Average Cross Validation Error: {average_error}, Average Accuracy: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(from_data, to_data):\n",
    "    return np.sqrt(np.sum((from_data[:, np.newaxis] - to_data[np.newaxis, :])**2, axis=2))\n",
    "\n",
    "def nearest_neighbour_classification(testing_inputs, training_inputs, training_outputs, no_of_neighbours):\n",
    "    distances = pairwise_distances(testing_inputs, training_inputs)\n",
    "    no_of_classes = 1 + np.max(training_outputs)\n",
    "    sorted_indices = np.argsort(distances, axis=1)\n",
    "\n",
    "    no_of_inputs = len(testing_inputs)\n",
    "    no_of_points = len(training_inputs)\n",
    "\n",
    "    new_array_to_sort = np.broadcast_to(training_outputs, (no_of_inputs, no_of_points))\n",
    "    sorted_labels = np.take_along_axis(new_array_to_sort, sorted_indices, 1)\n",
    "\n",
    "    predicted_labels = np.zeros(no_of_inputs, int)\n",
    "    for id_input in range(no_of_inputs):\n",
    "        probability_labels = np.zeros(no_of_classes, float)\n",
    "\n",
    "        for id_neighbor in range(0, no_of_neighbours):\n",
    "            probability_labels[sorted_labels[id_input][id_neighbor]] += 1. / no_of_neighbours\n",
    "\n",
    "        predicted_labels[id_input] = np.argmax(probability_labels)\n",
    "\n",
    "    return predicted_labels\n",
    "\n",
    "def classification_accuracy(estimated_labels, true_labels):\n",
    "    equal_labels = estimated_labels == true_labels\n",
    "    return np.mean(equal_labels)\n",
    "\n",
    "def KFold_split(data_size, K):\n",
    "    np.random.seed(123456789)\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K)\n",
    "    indexes_split = [\n",
    "        indexes[i * m + min(i, r):(i + 1) * m + min(i + 1, r)]\n",
    "        for i in range(K)\n",
    "    ]\n",
    "    return indexes_split\n",
    "\n",
    "def KFold_cross_validation_knn(data_inputs, data_outputs, K, labels_evaluation, missclassification_evaluation, knn):\n",
    "    data_size = len(data_inputs)\n",
    "    indexes_split = KFold_split(data_size, K)\n",
    "\n",
    "    average_accuracy = 0\n",
    "    for i in range(K):\n",
    "        training_indexes = np.concatenate([indexes_split[j] for j in range(K) if (j != i)])\n",
    "        \n",
    "        predicted_labels = labels_evaluation(data_inputs[indexes_split[i]],\n",
    "                                             data_inputs[training_indexes],\n",
    "                                             data_outputs[training_indexes], knn)\n",
    "        \n",
    "        accuracy = missclassification_evaluation(predicted_labels,\n",
    "                                                data_outputs[indexes_split[i]])\n",
    "        average_accuracy += accuracy / K\n",
    "        \n",
    "    error = 1. - average_accuracy\n",
    "    return error, 1. - error  \n",
    "\n",
    "\n",
    "def grid_search(objective, grid):\n",
    "    values = np.array([])\n",
    "    for point in grid:\n",
    "        values = np.append(values, objective(point))\n",
    "    return grid[np.argmin(values)]\n",
    "\n",
    "def objective_fun(Knn, data_inputs, data_outputs, K=5):\n",
    "    indexes_split = KFold_split(len(data_inputs), K)\n",
    "\n",
    "    average_accuracy = 0\n",
    "    for i in range(K):\n",
    "        training_indexes = np.concatenate([indexes_split[j] for j in range(K) if (j != i)])\n",
    "\n",
    "        predicted_labels = nearest_neighbour_classification(data_inputs[indexes_split[i]],\n",
    "                                                            data_inputs[training_indexes],\n",
    "                                                            data_outputs[training_indexes], Knn)\n",
    "\n",
    "        accuracy = classification_accuracy(predicted_labels, data_outputs[indexes_split[i]])\n",
    "        average_accuracy += accuracy / K\n",
    "        \n",
    "    error = 1. - average_accuracy\n",
    "    return error\n",
    "\n",
    "def run_knn_with_cross_validation(data_size, data_inputs, data_outputs, K=5):\n",
    "    k_values = np.arange(1, 10)\n",
    "    best_k = grid_search(lambda knn: objective_fun(knn, data_inputs, data_outputs, K=5), k_values)\n",
    "\n",
    "    cross_validation_errors = []\n",
    "    average_accuracies = []\n",
    "    for k in k_values:\n",
    "        error, accuracy = KFold_cross_validation_knn(data_inputs, data_outputs, K, nearest_neighbour_classification,\n",
    "                                                     classification_accuracy, k)\n",
    "        cross_validation_errors.append(error)\n",
    "        average_accuracies.append(accuracy)\n",
    "\n",
    "    return best_k, np.mean(cross_validation_errors), np.mean(average_accuracies)\n",
    "\n",
    "\n",
    "data_inputs = data_inputs\n",
    "data_outputs = churn_vector\n",
    "\n",
    "best_k, average_error, average_accuracy = run_knn_with_cross_validation(len(data_inputs), data_inputs, data_outputs, K=5)\n",
    "\n",
    "print(f'The best value for K is: {best_k}')\n",
    "print(f'Average Cross Validation Error: {average_error}, Average Accuracy: {average_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_inputs = data_inputs_final\n",
    "data_outputs = churn_vector\n",
    "\n",
    "best_k, average_error, average_accuracy = run_knn_with_cross_validation(len(data_inputs), data_inputs, data_outputs, K=5)\n",
    "\n",
    "print(f'The best value for K is: {best_k}')\n",
    "print(f'Average Cross Validation Error: {average_error}, Average Accuracy: {average_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a33f60",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e80d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_matrix(data_input):\n",
    "    if data_input.ndim == 1:\n",
    "        ones_column = np.ones((data_input.shape[0], 1))\n",
    "        return np.concatenate((ones_column, data_input.reshape(-1, 1)), axis=1)\n",
    "    else:\n",
    "        ones_column = np.ones((data_input.shape[0], 1))\n",
    "        return np.concatenate((ones_column, data_input), axis=1)\n",
    "\n",
    "def svm_train(X, y, learning_rate, iterations, C):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros((n, 1))\n",
    "    bias = 0\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        decision_function = np.dot(X, weights) + bias\n",
    "        margins = 1 - y * decision_function\n",
    "        loss = np.maximum(0, margins)\n",
    "        regularization = 0.5 * np.sum(weights[1:]**2) / m  # excluding bias term\n",
    "        total_loss = np.mean(loss) + C * regularization\n",
    "\n",
    "        gradient = -np.dot(X.T, y * (margins > 0)) / m + C * np.vstack([0, weights[1:]])\n",
    "        weights -= learning_rate * gradient\n",
    "        bias -= learning_rate * np.sum(y * (margins > 0)) / m\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {total_loss}\")\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def svm_predict(X, weights, bias):\n",
    "    return np.sign(np.dot(X, weights) + bias)\n",
    "\n",
    "def calculate_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def kfold_split(data_size, K):\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K)\n",
    "    indexes_split = [indexes[i * m + min(i, r):(i + 1) * m + min(i + 1, r)] for i in range(K)]\n",
    "    return indexes_split\n",
    "\n",
    "def kfold_cross_validation(X, y, k, svm_train, svm_predict, learning_rate, iterations, C):\n",
    "    n = len(y)\n",
    "    fold_size = n // k\n",
    "\n",
    "    accuracies = []\n",
    "    validation_errors = []\n",
    "\n",
    "    for i in range(k):\n",
    "        start, end = i * fold_size, (i + 1) * fold_size\n",
    "\n",
    "        X_test, y_test = X[start:end], y[start:end]\n",
    "        X_train, y_train = np.concatenate((X[:start], X[end:])), np.concatenate((y[:start], y[end:]))\n",
    "\n",
    "        weights, bias = svm_train(X_train, y_train, learning_rate, epochs, C)\n",
    "        predictions = svm_predict(X_test, weights, bias)\n",
    "\n",
    "        accuracy = calculate_accuracy(y_test, predictions)\n",
    "        validation_error = 1 - accuracy\n",
    "\n",
    "        accuracies.append(accuracy)\n",
    "        validation_errors.append(validation_error)\n",
    "\n",
    "        print(f\"Fold {i+1} Validation Error: {validation_error:.2%}\")\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    average_validation_error = np.mean(validation_errors)\n",
    "\n",
    "    print(f\"\\nAverage Accuracy: {average_accuracy:.2%}\")\n",
    "    print(f\"Average Validation Error: {average_validation_error:.2%}\")\n",
    "\n",
    "    return average_accuracy, average_validation_error\n",
    "\n",
    "def perform_grid_search(data_input, churn, learning_rates, C_values, k_folds, iterations):\n",
    "    best_accuracy = 0.0\n",
    "    best_validation_error = float('inf')\n",
    "    best_learning_rate = 0.0\n",
    "    best_C = 0.0\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for c_value in C_values:\n",
    "            print(f\"\\nTraining SVM with learning rate={lr} and C={c_value}\")\n",
    "\n",
    "            data_matrix = augment_data_matrix(data_input)\n",
    "\n",
    "            accuracy, validation_error = kfold_cross_validation(data_matrix, churn, k_folds, svm_train, svm_predict, lr, iterations, c_value)\n",
    "\n",
    "            if validation_error < best_validation_error:\n",
    "                best_accuracy = accuracy\n",
    "                best_validation_error = validation_error\n",
    "                best_learning_rate = lr\n",
    "                best_C = c_value\n",
    "\n",
    "    print(f\"\\nBest hyperparameters: Learning Rate={best_learning_rate}, C={best_C}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.2%}, Best Validation Error: {best_validation_error:.2%}\")\n",
    "\n",
    "\n",
    "data_inputs = age_vector\n",
    "churn = np.genfromtxt(bank_dataset, delimiter=\",\", skip_header=1, usecols=[11]).astype(int)\n",
    "churn = churn.reshape(-1, 1)\n",
    "#from reaserch these are the basic parameters to use\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "k_folds = 5\n",
    "iterations = 1000\n",
    "\n",
    "perform_grid_search(data_inputs, churn, learning_rates, C_values, k_folds, iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = activeMem  \n",
    "churn = np.genfromtxt(bank_dataset, delimiter=\",\", skip_header=1, usecols=[11]).astype(int)\n",
    "churn = churn.reshape(-1, 1)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "k_folds = 5\n",
    "iterations = 1000\n",
    "\n",
    "perform_grid_search(data_inputs, churn, learning_rates, C_values, k_folds, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = data_inputs \n",
    "churn = np.genfromtxt(bank_dataset, delimiter=\",\", skip_header=1, usecols=[11]).astype(int)\n",
    "churn = churn.reshape(-1, 1)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "k_folds = 5\n",
    "iterations = 1000\n",
    "\n",
    "perform_grid_search(data_inputs, churn, learning_rates, C_values, k_folds, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886cf283",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def destandardize_matrix(matrix, mean, std):\n",
    "    destandardized_matrix = matrix * std + mean\n",
    "    return destandardized_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4178b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_destandardized = destandardize_matrix(data_inputs_final, data_row_of_means, data_row_of_stds)\n",
    "data_inputs = X_destandardized\n",
    "churn = np.genfromtxt(bank_dataset, delimiter=\",\", skip_header=1, usecols=[11]).astype(int)\n",
    "churn = churn.reshape(-1, 1)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "k_folds = 5\n",
    "iterations = 1000\n",
    "\n",
    "perform_grid_search(data_inputs, churn, learning_rates, C_values, k_folds, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc9b3b",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d8d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to add coloumn of ones\n",
    "def gaussian_probability(x, mean, std):\n",
    "    # Gaussian probability density function\n",
    "    exponent = np.exp(-((x - mean)**2) / (2 * std**2))\n",
    "    return (1 / (np.sqrt(2 * np.pi) * std)) * exponent\n",
    "\n",
    "def train_naive_bayes(X, y):\n",
    "    class_priors = {}\n",
    "    unique_classes, counts = np.unique(y, return_counts=True)\n",
    "    total_samples = len(y)\n",
    "    for c, count in zip(unique_classes, counts):\n",
    "        class_priors[c] = count / total_samples\n",
    "\n",
    "    means = {}\n",
    "    stds = {}\n",
    "    for c in unique_classes:\n",
    "        class_samples = X[y == c]\n",
    "        means[c] = np.mean(class_samples, axis=0)\n",
    "        stds[c] = np.std(class_samples, axis=0)\n",
    "\n",
    "    return class_priors, means, stds\n",
    "\n",
    "def predict_naive_bayes(X, class_priors, means, stds):\n",
    "    predictions = []\n",
    "    for x in X:\n",
    "        class_scores = {}\n",
    "        for c in class_priors:\n",
    "            log_likelihood = np.sum(np.log(gaussian_probability(x, means[c], stds[c])))\n",
    "            class_scores[c] = log_likelihood + np.log(class_priors[c])\n",
    "\n",
    "        predicted_class = max(class_scores, key=class_scores.get)\n",
    "        predictions.append(predicted_class)\n",
    "\n",
    "    return np.array(predictions)\n",
    "\n",
    "def kfold_split(data_size, K):\n",
    "    indexes = np.random.permutation(data_size)\n",
    "    m, r = divmod(data_size, K)\n",
    "    indexes_split = [indexes[i * m + min(i, r):(i + 1) * m + min(i + 1, r)] for i in range(K)]\n",
    "    return indexes_split\n",
    "\n",
    "def kfold_cross_validation(X, y, k, train_func, predict_func):\n",
    "    indexes_split = kfold_split(len(y), k)\n",
    "    accuracies = []\n",
    "    errors = []\n",
    "\n",
    "    for i in range(k):\n",
    "        train_indexes = np.concatenate([indexes_split[j] for j in range(k) if j != i])\n",
    "        test_indexes = indexes_split[i]\n",
    "        X_train, y_train = X[train_indexes], y[train_indexes]\n",
    "        X_test, y_test = X[test_indexes], y[test_indexes]\n",
    "        class_priors, means, stds = train_func(X_train, y_train)\n",
    "        predictions = predict_func(X_test, class_priors, means, stds)\n",
    "        accuracy = np.mean(predictions == y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        error = 1 - accuracy\n",
    "        errors.append(error)\n",
    "        print(f\"Fold {i + 1} Accuracy: {accuracy:.2%}, Validation Error: {error:.2%}\")\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    average_error = np.mean(errors)\n",
    "\n",
    "    print(f\"\\nAverage Accuracy (K-Fold): {average_accuracy:.2%}\")\n",
    "    print(f\"Average Validation Error (K-Fold): {average_error:.2%}\")\n",
    "\n",
    "    return average_accuracy, average_error\n",
    "\n",
    "age1 = np.genfromtxt(bank_dataset, delimiter=\",\", skip_header=1, usecols=[4]).astype(float)\n",
    "data_inputs = age1\n",
    "y = churn_vector\n",
    "K = 5\n",
    "\n",
    "average_accuracy, average_error = kfold_cross_validation(data_inputs, y, K, train_naive_bayes, predict_naive_bayes)\n",
    "print(f'Overall Average Accuracy (K-Fold): {average_accuracy:.2%}')\n",
    "print(f'Overall Average Validation Error (K-Fold): {average_error:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "activeMem1=np.genfromtxt(bank_dataset,delimiter=\",\",skip_header=1,usecols=[9]).astype(float)\n",
    "data_inputs=activeMem1\n",
    "y=churn_vector\n",
    "K = 5\n",
    "\n",
    "average_accuracy, average_error = kfold_cross_validation(data_inputs, y, K, train_naive_bayes, predict_naive_bayes)\n",
    "print(f'Overall Average Accuracy (K-Fold): {average_accuracy:.2%}')\n",
    "print(f'Overall Average Validation Error (K-Fold): {average_error:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6486cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs = np.c_[age1,activeMem]\n",
    "y=churn_vector\n",
    "K = 5\n",
    "\n",
    "average_accuracy, average_error = kfold_cross_validation(data_inputs, y, K, train_naive_bayes, predict_naive_bayes)\n",
    "print(f'Overall Average Accuracy (K-Fold): {average_accuracy:.2%}')\n",
    "print(f'Overall Average Validation Error (K-Fold): {average_error:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6efa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs=np.c_[age,balance,productsNum,activeMem,gender]\n",
    "y=churn_vector\n",
    "\n",
    "K = 5\n",
    "\n",
    "average_accuracy, average_error = kfold_cross_validation(data_inputs, y, K, train_naive_bayes, predict_naive_bayes)\n",
    "print(f'Overall Average Accuracy (K-Fold): {average_accuracy:.2%}')\n",
    "print(f'Overall Average Validation Error (K-Fold): {average_error:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inputs_final=np.c_[age,balance,productsNum,activeMem,creditScore,estimatedSalary,gender]\n",
    "\n",
    "K = 5\n",
    "\n",
    "average_accuracy, average_error = kfold_cross_validation(data_inputs, y, K, train_naive_bayes, predict_naive_bayes)\n",
    "print(f'Overall Average Accuracy (K-Fold): {average_accuracy:.2%}')\n",
    "print(f'Overall Average Validation Error (K-Fold): {average_error:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d9ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
